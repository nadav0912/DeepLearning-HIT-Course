{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a121ed71-a872-4464-a8de-690eb44df7a8",
   "metadata": {},
   "source": [
    "## Homework #1: Prove the Gradients of the Sigmoid and Tanh Functions\n",
    "\n",
    "In this exercise, you are tasked with deriving the gradients of two common activation functions in deep learning: the sigmoid function and the tanh function. Prove each gradient and simplify your answers as much as possible.\n",
    "\n",
    "### 1. Sigmoid Function Gradient\n",
    "\n",
    "The sigmoid function is defined as:\n",
    "\n",
    "$$\n",
    "\\sigma(x) = \\frac{1}{1 + e^{-x}}\n",
    "$$\n",
    "\n",
    "#### Task\n",
    "1. Differentiate $\\sigma(x)$ with respect to $x$.\n",
    "2. Show that the derivative of $\\sigma(x)$ can be expressed in terms of $\\sigma(x)$ itself:\n",
    "   \n",
    "   $$\n",
    "   \\frac{d\\sigma(x)}{dx} = \\sigma(x)(1 - \\sigma(x))\n",
    "   $$\n",
    "\n",
    "**Hint:** To simplify your work, start by applying the chain rule to the expression $\\sigma(x) = \\left(1 + e^{-x}\\right)^{-1}$.\n",
    "\n",
    "### 2. Tanh Function Gradient\n",
    "\n",
    "The hyperbolic tangent function is defined as:\n",
    "\n",
    "$$\n",
    "\\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}\n",
    "$$\n",
    "\n",
    "#### Task\n",
    "1. Differentiate $\\tanh(x)$ with respect to $x$.\n",
    "2. Show that the derivative of $\\tanh(x)$ can be expressed as:\n",
    "\n",
    "   $$\n",
    "   \\frac{d\\tanh(x)}{dx} = 1 - \\tanh(x)^2\n",
    "   $$\n",
    "\n",
    "**Hint:** You may want to express $\\tanh(x)$ in terms of $\\sigma(x)$, noting that $\\tanh(x) = 2\\sigma(2x) - 1$, as an alternative approach.\n",
    "\n",
    "### Submission\n",
    "\n",
    "1. Write out the derivations step-by-step.\n",
    "2. Simplify each result as much as possible.\n",
    "3. Confirm that your results mat4. Complete the derivations in a Jupyter notebook. Use markdown cells continaing Latex code.\n",
    "5. Submit your notebook by uploading it to either Google Colab or GitHub.\n",
    "6. Share the link to your notebook with idan.tobis@gmail.com.ch the expected gradient forms given above.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c3378a",
   "metadata": {},
   "source": [
    "### Solution 1: Sigmoid Function Gradient\n",
    "\n",
    "$$\n",
    "\\frac{d\\sigma(x)}{dx} = (-1)*(1+e^{-x})^{-2}e^{-x}*(-1) = \\frac{e^{-x}}{(1+e^{-x})^{-2}} = \\frac{e^{-x}}{(1+e^{-x})(1+e^{-x})}\n",
    "$$\n",
    "\n",
    "$$\n",
    "= (\\frac{1}{1+e^{-x}}) * (\\frac{e^{-x}}{1+e^{-x}} - 1 + 1) = \\sigma(x) * (\\frac{e^{-x}}{1+e^{-x}} - \\frac{1 + e^{-x}}{1+e^{-x}} + 1)\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\sigma(x) * (\\frac{e^{-x} - 1 - e^{-x}}{1+e^{-x}} + 1) = \\sigma(x) * (\\frac{-1}{1 + e^{-x}} + 1) = \\sigma(x) * (1 - \\sigma(x))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c073127",
   "metadata": {},
   "source": [
    "### Solution 2. Tanh Function Gradient\n",
    "\n",
    "$$\n",
    "\\frac{d\\tanh(x)}{dx} = \\frac{\\frac{d(e^{x} - e^{-x})}{dx} * (e^{x} + e^{-x}) - \\frac{d(e^{x} + e^{-x})}{dx} * (e^{x} - e^{-x})}{(e^{x} + e^{-x})^{2}}\n",
    "$$\n",
    "$$\n",
    "    = \\frac{(e^{x} + e^{-x}) * (e^{x} + e^{-x}) - (e^{x} - e^{-x}) * (e^{x} - e^{-x})}{(e^{x} + e^{-x})^{2}}\n",
    "$$\n",
    "$$\n",
    "    = \\frac{(e^{x} + e^{-x})^{2}}{(e^{x} + e^{-x})^{2}} - \\frac{(e^{x} - e^{-x})^{2}}{(e^{x} + e^{-x})^{2}}\n",
    "$$\n",
    "$$\n",
    "    = 1 - (\\frac{e^{x} - e^{-x}}{e^{x} + e^{-x}})^{2} = 1 - (\\tanh(x))^{2}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a23216",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
